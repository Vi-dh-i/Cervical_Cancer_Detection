# -*- coding: utf-8 -*-
"""Cervical_Cancer_Detections.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uTvBm0c_F8Ij5l5WuSY2v8L0_cDwQEjT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
df = pd.read_csv('/content/cervical-cancer.csv')
df.head()

df.tail()
# disription of all the atribut

df.info()

df.shape

df.columns

df.duplicated().sum()

df.isnull().sum()

df.columns = df.columns.str.strip()
df = df.drop(columns=['STDs_Time_since_first_diagnosis', 'STDs_Time_since_last_diagnosis'], errors='ignore')
print(df)
df.info()

df.describe()

df.columns

from sklearn.impute import SimpleImputer

# Define num_features to include the numerical columns of the DataFrame
num_features = df.select_dtypes(include=np.number).columns.tolist()

# The rest of your code remains the same
imputer_num = SimpleImputer(strategy='median')
df[num_features] = imputer_num.fit_transform(df[num_features])
df

df.isnull().sum()

object_columns = df.select_dtypes(include=['object']).columns
print("Object type columns:")
print(object_columns)
numerical_columns = df.select_dtypes(include=['int64',
'float64']).columns
print("\nNumerical type columns:")
print(numerical_columns)

def classify_features(df):
    categorical_features = []
    non_categorical_features = []
    discrete_features = []
    continuous_features = []
    for column in df.columns:
        if df[column].dtype == 'object':
            if df[column].nunique() < 10:
                categorical_features.append(column)
            else:
                non_categorical_features.append(column)
        elif df[column].dtype in ['int64', 'float64']:
            if df[column].nunique() < 10:
                discrete_features.append(column)
            else:
                continuous_features.append(column)
    return categorical_features, non_categorical_features, discrete_features, continuous_features # Corrected indentation here

categorical, non_categorical, discrete, continuous = classify_features(df)
print("Categorical Features:", categorical)
print("Non-Categorical Features:", non_categorical)
print("Discrete Features:", discrete)
print("Continuous Features:", continuous)

for i in discrete:
 print(i)
 print(df[i].unique())
 print()

for i in discrete:
 print(df[i].value_counts())
 print()

for i in discrete:
    plt.figure(figsize=(15, 6))
    ax = sns.countplot(x=i, data=df, palette='hls')
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(f'{height}',
                    xy=(p.get_x() + p.get_width() / 2., height),
                    xytext=(0, 10),
                    textcoords='offset points',
                    ha='center', va='center')

    plt.show()

import plotly.express as px
for i in discrete:
 counts = df[i].value_counts()
 fig = px.pie(counts, values=counts.values, names=counts.index,
title=f'Distribution of {i}')
 fig.show()

for i in continuous:
 plt.figure(figsize=(15,6))
 sns.histplot(df[i], bins = 20, kde = True, palette='hls')
 plt.xticks(rotation = 90)
 plt.show()

for i in continuous:
 plt.figure(figsize=(15,6))
 sns.distplot(df[i], bins = 20, kde = True)
 plt.xticks(rotation = 90)
 plt.show()

for i in continuous:
 plt.figure(figsize=(15, 6))
 sns.boxplot(x=i, data=df, palette='hls')
 plt.xticks(rotation=90)
 plt.show()

for i in range(len(continuous)):
    for j in range(i + 1, len(continuous)):
        plt.figure(figsize=(15, 6)) # Indented this line to be part of the outer 'for' loop
        # Rest of the code remains indented as it is
        for i in range(len(continuous)):
            for j in range(i + 1, len(continuous)):
                plt.figure(figsize=(15, 6))
                sns.scatterplot(x=continuous[i], y=continuous[j], data=df,
                palette='hls')
                plt.title(f'Scatter plot of {continuous[i]} vs {continuous[j]}')
                plt.show()

for dis in discrete:
    for cont in continuous: # Corrected: Indented this line by 4 spaces
        plt.figure(figsize=(10, 6))
        ax = sns.barplot(data=df, x=dis, y=cont, ci=None)
        plt.title(f'{dis} vs {cont}')

        for p in ax.patches:
            height = p.get_height()
            ax.annotate(f'{height:.2f}', (p.get_x() + p.get_width() / 2., height),
                        ha='center', va='bottom', fontsize=10,
                        color='black', rotation=0)
        plt.xticks(rotation=90)
        plt.show()

corr_matrix = df[continuous].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f",
linewidths=0.5)
plt.title('Correlation Plot of Continuous Features')
plt.show()

corr_matrix

threshold = 0.7
features_to_drop = []
for i in range(len(corr_matrix.columns)):
    for j in range(i): # This was missing the indentation
        if abs(corr_matrix.iloc[i, j]) > threshold: # This was missing the indentation
            colname = corr_matrix.columns[i] # This was missing the indentation
            if colname not in features_to_drop: # This was missing the indentation
                features_to_drop.append(colname) # This was missing the indentation
features_to_drop
['Smokes (packs/year)']
df = df.drop(columns=features_to_drop)
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
X = df.drop(columns=['Dx:Cancer'])
y = df['Dx:Cancer']
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.2, stratify = y, random_state=42)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
classifier = RandomForestClassifier(random_state=42)
classifier.fit(X_resampled, y_resampled)
RandomForestClassifier(random_state=42)
y_pred = classifier.predict(X_test)
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_resampled, y_resampled)
DecisionTreeClassifier(random_state=42)
y_pred_dt = dt_classifier.predict(X_test)
print("Decision Tree Classifier")
print("Classification Report:")
print(classification_report(y_test, y_pred_dt))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))

# ... (previous code) ...

# Initialize and train the Logistic Regression model
log_reg_classifier = LogisticRegression(random_state=42) # Create a LogisticRegression object
log_reg_classifier.fit(X_resampled, y_resampled) # Train the model on the resampled data


y_pred_log_reg = log_reg_classifier.predict(X_test) # Now you can use the trained model for prediction
print("Logistic Regression")
print("Classification Report:")
print(classification_report(y_test, y_pred_log_reg))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_log_reg))

svc_classifier = SVC(random_state=42)
svc_classifier.fit(X_resampled, y_resampled)
SVC(random_state=42)
y_pred_svc = svc_classifier.predict(X_test)
print("Support Vector Classifier")
print("Classification Report:")
print(classification_report(y_test, y_pred_svc))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svc))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Initialize the classifier
xgb_classifier = XGBClassifier(
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Fit the classifier with resampled data
xgb_classifier.fit(X_resampled, y_resampled)

# Make predictions
y_pred_xgb = xgb_classifier.predict(X_test)

# Print results
print("XGBoost Classifier")
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))

classifiers = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Support Vector Classifier (SVC)': SVC(random_state=42),
    'XGBoost Classifier': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
    'Random Forest Classifier': RandomForestClassifier(random_state=42)
}
from sklearn.metrics import accuracy_score
results = {}
for name, clf in classifiers.items():
    clf.fit(X_resampled, y_resampled)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    cm = confusion_matrix(y_test, y_pred)

    # This block was unnecessarily repeated, removed the duplicate
    # The heatmap plotting and confusion matrix should be within the loop to show results for each classifier
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Cancer', 'Cancer'], yticklabels=['No Cancer', 'Cancer'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')  # Corrected indentation here
    plt.title(f'Confusion Matrix - {name}')
    plt.show()

# Plot the accuracies after the loop has finished
plt.figure(figsize=(12, 6))
plt.bar(results.keys(), results.values(), color=['blue', 'green', 'red', 'purple', 'orange'])
plt.xlabel('Classifiers')
plt.ylabel('Accuracy')
plt.title('Comparison of Model Accuracies')
plt.ylim(0, 1.3)
plt.show()

from sklearn.metrics import accuracy_score

# Dictionary to store results for each classifier
results = {}

# Iterate through classifiers
for name, clf in classifiers.items():
    # Fit the classifier
    clf.fit(X_resampled, y_resampled)

    # Predict on the test data
    y_pred = clf.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy

    # Print accuracy and classification report
    print(f"{name} Accuracy: {accuracy:.2f}")
    print(f"{name} Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix and Heatmap
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Cancer', 'Cancer'], yticklabels=['No Cancer', 'Cancer'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'{name} - Confusion Matrix (Accuracy: {accuracy:.2f})')
    plt.show()

# Bar plot to compare accuracies
plt.figure(figsize=(12, 6))
plt.bar(results.keys(), results.values(), color=['blue', 'green', 'red', 'purple', 'orange'])
plt.xlabel('Classifiers')
plt.ylabel('Accuracy')
plt.title('Comparison of Model Accuracies')
plt.ylim(0, 1.3)
plt.show()